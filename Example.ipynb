{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caee6449",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, input_dim=9, hidden_dim=7, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build encoder and decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build the encoder network: input(9) -> hidden(7) -> latent(2)\"\"\"\n",
    "        inputs = keras.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden = layers.Dense(self.hidden_dim, activation=\"relu\")(inputs)\n",
    "        \n",
    "        # Mean and log variance for latent space\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(hidden)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(hidden)\n",
    "        \n",
    "        return keras.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build the decoder network: latent(2) -> hidden(7) -> output(9)\"\"\"\n",
    "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden = layers.Dense(self.hidden_dim, activation=\"relu\")(latent_inputs)\n",
    "        \n",
    "        # Output layer (sigmoid for values between 0 and 1)\n",
    "        outputs = layers.Dense(self.input_dim, activation=\"sigmoid\")(hidden)\n",
    "        \n",
    "        return keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick: sample from N(mu, sigma) using N(0,1)\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass through the VAE\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent space\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        return self.reparameterize(z_mean, z_log_var)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode from latent space\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "class VAETrainer:\n",
    "    def __init__(self, vae, optimizer=None):\n",
    "        self.vae = vae\n",
    "        self.optimizer = optimizer or keras.optimizers.Adam(1e-3)\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        \"\"\"Compute VAE loss: reconstruction loss + KL divergence\"\"\"\n",
    "        reconstructed, z_mean, z_log_var = self.vae(x)\n",
    "        \n",
    "        # Reconstruction loss (mean squared error for continuous data)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(tf.square(x - reconstructed), axis=1)\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, reconstruction_loss, kl_loss = self.compute_loss(x)\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.vae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.vae.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, epochs=100, verbose=1):\n",
    "        \"\"\"Train the VAE\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Reset metrics\n",
    "            self.total_loss_tracker.reset_states()\n",
    "            self.reconstruction_loss_tracker.reset_states()\n",
    "            self.kl_loss_tracker.reset_states()\n",
    "            \n",
    "            # Training loop\n",
    "            for step, x_batch in enumerate(dataset):\n",
    "                metrics = self.train_step(x_batch)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss: {metrics['loss']:.4f}, \"\n",
    "                      f\"Reconstruction: {metrics['reconstruction_loss']:.4f}, \"\n",
    "                      f\"KL: {metrics['kl_loss']:.4f}\")\n",
    "\n",
    "def generate_synthetic_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic 9-dimensional data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create two clusters in 9D space\n",
    "    cluster1 = np.random.multivariate_normal(\n",
    "        mean=[0.3, 0.7, 0.2, 0.8, 0.1, 0.9, 0.4, 0.6, 0.5],\n",
    "        cov=np.eye(9) * 0.05,\n",
    "        size=n_samples // 2\n",
    "    )\n",
    "    \n",
    "    cluster2 = np.random.multivariate_normal(\n",
    "        mean=[0.8, 0.2, 0.9, 0.1, 0.7, 0.3, 0.6, 0.4, 0.5],\n",
    "        cov=np.eye(9) * 0.05,\n",
    "        size=n_samples // 2\n",
    "    )\n",
    "    \n",
    "    # Combine and normalize to [0, 1]\n",
    "    data = np.vstack([cluster1, cluster2])\n",
    "    data = np.clip(data, 0, 1)  # Ensure values are in [0, 1]\n",
    "    \n",
    "    # Create labels for visualization\n",
    "    labels = np.concatenate([np.zeros(n_samples // 2), np.ones(n_samples // 2)])\n",
    "    \n",
    "    return data.astype(np.float32), labels.astype(int)\n",
    "\n",
    "def plot_latent_space(vae, data, labels=None, title=\"Latent Space Representation\"):\n",
    "    \"\"\"Plot the 2D latent space representation\"\"\"\n",
    "    # Encode to latent space\n",
    "    z_mean, _ = vae.encoder(data)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "    else:\n",
    "        plt.scatter(z_mean[:, 0], z_mean[:, 1], alpha=0.6)\n",
    "    \n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstructions(vae, data, n_samples=5):\n",
    "    \"\"\"Plot original vs reconstructed vectors\"\"\"\n",
    "    indices = np.random.choice(len(data), n_samples, replace=False)\n",
    "    x_sample = data[indices]\n",
    "    reconstructions, _, _ = vae(x_sample)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original\n",
    "        axes[0, i].bar(range(9), x_sample[i])\n",
    "        axes[0, i].set_title(f\"Original {i+1}\")\n",
    "        axes[0, i].set_ylim(0, 1)\n",
    "        axes[0, i].set_xticks(range(9))\n",
    "        \n",
    "        # Reconstruction\n",
    "        axes[1, i].bar(range(9), reconstructions[i])\n",
    "        axes[1, i].set_title(f\"Reconstructed {i+1}\")\n",
    "        axes[1, i].set_ylim(0, 1)\n",
    "        axes[1, i].set_xticks(range(9))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_generated_samples(vae, n_samples=5):\n",
    "    \"\"\"Generate and plot new samples from random latent vectors\"\"\"\n",
    "    # Sample random points in latent space\n",
    "    random_latent = tf.random.normal(shape=(n_samples, vae.latent_dim))\n",
    "    generated_samples = vae.decode(random_latent)\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i + 1)\n",
    "        plt.bar(range(9), generated_samples[i])\n",
    "        plt.title(f\"Generated {i+1}\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(range(9))\n",
    "    \n",
    "    plt.suptitle(\"Generated Samples from Random Latent Vectors\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def interpolate_in_latent_space(vae, data, n_steps=5):\n",
    "    \"\"\"Interpolate between two points in latent space\"\"\"\n",
    "    # Get two random samples\n",
    "    indices = np.random.choice(len(data), 2, replace=False)\n",
    "    x1, x2 = data[indices[0]:indices[0]+1], data[indices[1]:indices[1]+1]\n",
    "    \n",
    "    # Encode to latent space\n",
    "    z1_mean, _ = vae.encoder(x1)\n",
    "    z2_mean, _ = vae.encoder(x2)\n",
    "    \n",
    "    # Interpolate\n",
    "    alphas = np.linspace(0, 1, n_steps)\n",
    "    interpolated_z = []\n",
    "    for alpha in alphas:\n",
    "        z_interp = (1 - alpha) * z1_mean + alpha * z2_mean\n",
    "        interpolated_z.append(z_interp)\n",
    "    \n",
    "    interpolated_z = tf.concat(interpolated_z, axis=0)\n",
    "    interpolated_samples = vae.decode(interpolated_z)\n",
    "    \n",
    "    # Plot interpolation\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n_steps):\n",
    "        plt.subplot(1, n_steps, i + 1)\n",
    "        plt.bar(range(9), interpolated_samples[i])\n",
    "        plt.title(f\"α = {alphas[i]:.2f}\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(range(9))\n",
    "    \n",
    "    plt.suptitle(\"Latent Space Interpolation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_model_summary(vae):\n",
    "    \"\"\"Print model architecture summary\"\"\"\n",
    "    print(\"VAE Architecture:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Input dimension: {vae.input_dim}\")\n",
    "    print(f\"Hidden dimension: {vae.hidden_dim}\")\n",
    "    print(f\"Latent dimension: {vae.latent_dim}\")\n",
    "    print(\"\\nEncoder:\")\n",
    "    vae.encoder.summary()\n",
    "    print(\"\\nDecoder:\")\n",
    "    vae.decoder.summary()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    print(\"Generating synthetic 9D data...\")\n",
    "    data, labels = generate_synthetic_data(n_samples=2000)\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_size = int(0.8 * len(data))\n",
    "    x_train, x_test = data[:train_size], data[train_size:]\n",
    "    y_train, y_test = labels[:train_size], labels[train_size:]\n",
    "    \n",
    "    # Create dataset\n",
    "    batch_size = 64\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "    \n",
    "    # Create VAE\n",
    "    vae = VAE(input_dim=9, hidden_dim=7, latent_dim=2)\n",
    "    print_model_summary(vae)\n",
    "    \n",
    "    # Train VAE\n",
    "    trainer = VAETrainer(vae)\n",
    "    print(\"\\nTraining VAE...\")\n",
    "    trainer.train(train_dataset, epochs=100)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\nPlotting reconstructions...\")\n",
    "    plot_reconstructions(vae, x_test)\n",
    "    \n",
    "    print(\"Plotting generated samples...\")\n",
    "    plot_generated_samples(vae)\n",
    "    \n",
    "    print(\"Plotting latent space...\")\n",
    "    plot_latent_space(vae, x_test, y_test)\n",
    "    \n",
    "    print(\"Plotting latent space interpolation...\")\n",
    "    interpolate_in_latent_space(vae, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, latent_dim=32, input_shape=(28, 28, 1)):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Build encoder and decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build the encoder network\"\"\"\n",
    "        inputs = keras.Input(shape=self.input_shape)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(16, activation=\"relu\")(x)\n",
    "        \n",
    "        # Mean and log variance for latent space\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "        \n",
    "        return keras.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build the decoder network\"\"\"\n",
    "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
    "        x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "        x = layers.Reshape((7, 7, 64))(x)\n",
    "        \n",
    "        # Transpose convolutional layers\n",
    "        x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "        x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "        \n",
    "        return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick: sample from N(mu, sigma) using N(0,1)\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass through the VAE\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent space\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        return self.reparameterize(z_mean, z_log_var)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode from latent space\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "class VAETrainer:\n",
    "    def __init__(self, vae, optimizer=None):\n",
    "        self.vae = vae\n",
    "        self.optimizer = optimizer or keras.optimizers.Adam(1e-4)\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        \"\"\"Compute VAE loss: reconstruction loss + KL divergence\"\"\"\n",
    "        reconstructed, z_mean, z_log_var = self.vae(x)\n",
    "        \n",
    "        # Reconstruction loss (binary crossentropy)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(x, reconstructed), axis=(1, 2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, reconstruction_loss, kl_loss = self.compute_loss(x)\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.vae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.vae.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, epochs=10, verbose=1):\n",
    "        \"\"\"Train the VAE\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Reset metrics\n",
    "            self.total_loss_tracker.reset_states()\n",
    "            self.reconstruction_loss_tracker.reset_states()\n",
    "            self.kl_loss_tracker.reset_states()\n",
    "            \n",
    "            # Training loop\n",
    "            for step, x_batch in enumerate(dataset):\n",
    "                metrics = self.train_step(x_batch)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss: {metrics['loss']:.4f}, \"\n",
    "                      f\"Reconstruction: {metrics['reconstruction_loss']:.4f}, \"\n",
    "                      f\"KL: {metrics['kl_loss']:.4f}\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess MNIST dataset\"\"\"\n",
    "    (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize to [0, 1] and add channel dimension\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "def plot_latent_space(vae, x_test, y_test=None, n_samples=1000):\n",
    "    \"\"\"Plot the latent space representation\"\"\"\n",
    "    # Sample subset of test data\n",
    "    indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "    x_sample = x_test[indices]\n",
    "    \n",
    "    # Encode to latent space\n",
    "    z_mean, _ = vae.encoder(x_sample)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if y_test is not None:\n",
    "        y_sample = y_test[indices]\n",
    "        scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_sample, cmap='tab10', alpha=0.6)\n",
    "        plt.colorbar(scatter)\n",
    "    else:\n",
    "        plt.scatter(z_mean[:, 0], z_mean[:, 1], alpha=0.6)\n",
    "    \n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(\"Latent Space Representation\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_generated_images(vae, n_samples=10):\n",
    "    \"\"\"Generate and plot new images from random latent vectors\"\"\"\n",
    "    # Sample random points in latent space\n",
    "    random_latent = tf.random.normal(shape=(n_samples, vae.latent_dim))\n",
    "    generated_images = vae.decode(random_latent)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i + 1)\n",
    "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Generated Images from Random Latent Vectors\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstructions(vae, x_test, n_samples=10):\n",
    "    \"\"\"Plot original vs reconstructed images\"\"\"\n",
    "    indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "    x_sample = x_test[indices]\n",
    "    reconstructions, _, _ = vae(x_sample)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n_samples):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n_samples, i + 1)\n",
    "        plt.imshow(x_sample[i, :, :, 0], cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        ax = plt.subplot(2, n_samples, i + 1 + n_samples)\n",
    "        plt.imshow(reconstructions[i, :, :, 0], cmap='gray')\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    x_train, x_test = load_and_preprocess_data()\n",
    "    \n",
    "    # Create dataset\n",
    "    batch_size = 128\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Create and train VAE\n",
    "    vae = VAE(latent_dim=2)  # 2D latent space for visualization\n",
    "    trainer = VAETrainer(vae)\n",
    "    \n",
    "    print(\"Training VAE...\")\n",
    "    trainer.train(train_dataset, epochs=10)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"Plotting reconstructions...\")\n",
    "    plot_reconstructions(vae, x_test)\n",
    "    \n",
    "    print(\"Plotting generated images...\")\n",
    "    plot_generated_images(vae)\n",
    "    \n",
    "    # Load labels for latent space visualization\n",
    "    (_, y_train), (_, y_test) = keras.datasets.mnist.load_data()\n",
    "    print(\"Plotting latent space...\")\n",
    "    plot_latent_space(vae, x_test, y_test)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dd784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class CVAE(keras.Model):\n",
    "    def __init__(self, input_dim=9, hidden_dim=7, latent_dim=2, num_classes=3):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build encoder and decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build the conditional encoder network: input(9) + condition(num_classes) -> hidden(7) -> latent(2)\"\"\"\n",
    "        # Input data\n",
    "        data_inputs = keras.Input(shape=(self.input_dim,), name=\"data_input\")\n",
    "        \n",
    "        # Condition input (one-hot encoded)\n",
    "        condition_inputs = keras.Input(shape=(self.num_classes,), name=\"condition_input\")\n",
    "        \n",
    "        # Concatenate data and condition\n",
    "        combined = layers.Concatenate()([data_inputs, condition_inputs])\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden = layers.Dense(self.hidden_dim, activation=\"relu\")(combined)\n",
    "        \n",
    "        # Mean and log variance for latent space\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(hidden)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(hidden)\n",
    "        \n",
    "        return keras.Model([data_inputs, condition_inputs], [z_mean, z_log_var], name=\"encoder\")\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build the conditional decoder network: latent(2) + condition(num_classes) -> hidden(7) -> output(9)\"\"\"\n",
    "        # Latent input\n",
    "        latent_inputs = keras.Input(shape=(self.latent_dim,), name=\"latent_input\")\n",
    "        \n",
    "        # Condition input (one-hot encoded)\n",
    "        condition_inputs = keras.Input(shape=(self.num_classes,), name=\"condition_input\")\n",
    "        \n",
    "        # Concatenate latent and condition\n",
    "        combined = layers.Concatenate()([latent_inputs, condition_inputs])\n",
    "        \n",
    "        # Hidden layer\n",
    "        hidden = layers.Dense(self.hidden_dim, activation=\"relu\")(combined)\n",
    "        \n",
    "        # Output layer (sigmoid for values between 0 and 1)\n",
    "        outputs = layers.Dense(self.input_dim, activation=\"sigmoid\")(hidden)\n",
    "        \n",
    "        return keras.Model([latent_inputs, condition_inputs], outputs, name=\"decoder\")\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick: sample from N(mu, sigma) using N(0,1)\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass through the CVAE\"\"\"\n",
    "        data, conditions = inputs\n",
    "        z_mean, z_log_var = self.encoder([data, conditions])\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decoder([z, conditions])\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def encode(self, data, conditions):\n",
    "        \"\"\"Encode input to latent space\"\"\"\n",
    "        z_mean, z_log_var = self.encoder([data, conditions])\n",
    "        return self.reparameterize(z_mean, z_log_var)\n",
    "    \n",
    "    def decode(self, z, conditions):\n",
    "        \"\"\"Decode from latent space\"\"\"\n",
    "        return self.decoder([z, conditions])\n",
    "\n",
    "class CVAETrainer:\n",
    "    def __init__(self, cvae, optimizer=None):\n",
    "        self.cvae = cvae\n",
    "        self.optimizer = optimizer or keras.optimizers.Adam(1e-3)\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    def compute_loss(self, data, conditions):\n",
    "        \"\"\"Compute CVAE loss: reconstruction loss + KL divergence\"\"\"\n",
    "        reconstructed, z_mean, z_log_var = self.cvae([data, conditions])\n",
    "        \n",
    "        # Reconstruction loss (mean squared error for continuous data)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(tf.square(data - reconstructed), axis=1)\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data, conditions):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, reconstruction_loss, kl_loss = self.compute_loss(data, conditions)\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.cvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.cvae.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, epochs=100, verbose=1):\n",
    "        \"\"\"Train the CVAE\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Reset metrics\n",
    "            self.total_loss_tracker.reset_states()\n",
    "            self.reconstruction_loss_tracker.reset_states()\n",
    "            self.kl_loss_tracker.reset_states()\n",
    "            \n",
    "            # Training loop\n",
    "            for step, (data_batch, condition_batch) in enumerate(dataset):\n",
    "                metrics = self.train_step(data_batch, condition_batch)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss: {metrics['loss']:.4f}, \"\n",
    "                      f\"Reconstruction: {metrics['reconstruction_loss']:.4f}, \"\n",
    "                      f\"KL: {metrics['kl_loss']:.4f}\")\n",
    "\n",
    "def generate_conditional_synthetic_data(n_samples=1500, num_classes=3):\n",
    "    \"\"\"Generate synthetic 9-dimensional data with different conditions/classes\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Define different patterns for each class\n",
    "    class_patterns = [\n",
    "        # Class 0: Low values pattern\n",
    "        {\"mean\": [0.2, 0.3, 0.1, 0.4, 0.2, 0.3, 0.1, 0.2, 0.3], \"cov\": 0.03},\n",
    "        # Class 1: Medium values pattern  \n",
    "        {\"mean\": [0.5, 0.6, 0.4, 0.7, 0.5, 0.6, 0.4, 0.5, 0.6], \"cov\": 0.03},\n",
    "        # Class 2: High values pattern\n",
    "        {\"mean\": [0.8, 0.7, 0.9, 0.6, 0.8, 0.7, 0.9, 0.8, 0.7], \"cov\": 0.03}\n",
    "    ]\n",
    "    \n",
    "    samples_per_class = n_samples // num_classes\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        pattern = class_patterns[class_id]\n",
    "        \n",
    "        class_data = np.random.multivariate_normal(\n",
    "            mean=pattern[\"mean\"],\n",
    "            cov=np.eye(9) * pattern[\"cov\"],\n",
    "            size=samples_per_class\n",
    "        )\n",
    "        \n",
    "        # Clip to [0, 1] range\n",
    "        class_data = np.clip(class_data, 0, 1)\n",
    "        \n",
    "        data_list.append(class_data)\n",
    "        labels_list.append(np.full(samples_per_class, class_id))\n",
    "    \n",
    "    # Combine all classes\n",
    "    data = np.vstack(data_list).astype(np.float32)\n",
    "    labels = np.concatenate(labels_list).astype(int)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(len(data))\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def labels_to_onehot(labels, num_classes):\n",
    "    \"\"\"Convert integer labels to one-hot encoding\"\"\"\n",
    "    return tf.one_hot(labels, num_classes)\n",
    "\n",
    "def plot_conditional_latent_space(cvae, data, labels, title=\"Conditional Latent Space\"):\n",
    "    \"\"\"Plot the 2D latent space for different conditions\"\"\"\n",
    "    conditions_onehot = labels_to_onehot(labels, cvae.num_classes)\n",
    "    \n",
    "    # Encode to latent space\n",
    "    z_mean, _ = cvae.encoder([data, conditions_onehot])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for class_id in range(cvae.num_classes):\n",
    "        mask = labels == class_id\n",
    "        plt.scatter(z_mean[mask, 0], z_mean[mask, 1], \n",
    "                   c=colors[class_id], label=f'Class {class_id}', alpha=0.6)\n",
    "    \n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_conditional_reconstructions(cvae, data, labels, n_samples=5):\n",
    "    \"\"\"Plot original vs reconstructed vectors for each class\"\"\"\n",
    "    fig, axes = plt.subplots(2 * cvae.num_classes, n_samples, \n",
    "                            figsize=(15, 4 * cvae.num_classes))\n",
    "    \n",
    "    for class_id in range(cvae.num_classes):\n",
    "        # Get samples from this class\n",
    "        class_mask = labels == class_id\n",
    "        class_data = data[class_mask]\n",
    "        class_labels = labels[class_mask]\n",
    "        \n",
    "        if len(class_data) < n_samples:\n",
    "            continue\n",
    "            \n",
    "        indices = np.random.choice(len(class_data), n_samples, replace=False)\n",
    "        x_sample = class_data[indices]\n",
    "        y_sample = class_labels[indices]\n",
    "        \n",
    "        conditions_onehot = labels_to_onehot(y_sample, cvae.num_classes)\n",
    "        reconstructions, _, _ = cvae([x_sample, conditions_onehot])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            row_orig = class_id * 2\n",
    "            row_recon = class_id * 2 + 1\n",
    "            \n",
    "            # Original\n",
    "            axes[row_orig, i].bar(range(9), x_sample[i])\n",
    "            axes[row_orig, i].set_title(f\"Class {class_id} - Original {i+1}\")\n",
    "            axes[row_orig, i].set_ylim(0, 1)\n",
    "            axes[row_orig, i].set_xticks(range(9))\n",
    "            \n",
    "            # Reconstruction\n",
    "            axes[row_recon, i].bar(range(9), reconstructions[i])\n",
    "            axes[row_recon, i].set_title(f\"Class {class_id} - Reconstructed {i+1}\")\n",
    "            axes[row_recon, i].set_ylim(0, 1)\n",
    "            axes[row_recon, i].set_xticks(range(9))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_conditional_samples(cvae, n_samples_per_class=3):\n",
    "    \"\"\"Generate new samples for each condition\"\"\"\n",
    "    fig, axes = plt.subplots(cvae.num_classes, n_samples_per_class, \n",
    "                            figsize=(12, 3 * cvae.num_classes))\n",
    "    \n",
    "    for class_id in range(cvae.num_classes):\n",
    "        # Create condition vector for this class\n",
    "        conditions = np.zeros((n_samples_per_class, cvae.num_classes))\n",
    "        conditions[:, class_id] = 1  # One-hot encoding\n",
    "        conditions = tf.constant(conditions, dtype=tf.float32)\n",
    "        \n",
    "        # Sample random latent vectors\n",
    "        random_latent = tf.random.normal(shape=(n_samples_per_class, cvae.latent_dim))\n",
    "        \n",
    "        # Generate samples\n",
    "        generated_samples = cvae.decode(random_latent, conditions)\n",
    "        \n",
    "        for i in range(n_samples_per_class):\n",
    "            if cvae.num_classes == 1:\n",
    "                ax = axes[i]\n",
    "            else:\n",
    "                ax = axes[class_id, i]\n",
    "                \n",
    "            ax.bar(range(9), generated_samples[i])\n",
    "            ax.set_title(f\"Class {class_id} - Generated {i+1}\")\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks(range(9))\n",
    "    \n",
    "    plt.suptitle(\"Generated Samples by Condition\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def interpolate_between_conditions(cvae, data, labels):\n",
    "    \"\"\"Interpolate between different conditions in latent space\"\"\"\n",
    "    # Get one sample from each class\n",
    "    samples_by_class = []\n",
    "    for class_id in range(cvae.num_classes):\n",
    "        class_mask = labels == class_id\n",
    "        class_data = data[class_mask]\n",
    "        if len(class_data) > 0:\n",
    "            samples_by_class.append(class_data[0:1])  # Take first sample\n",
    "    \n",
    "    if len(samples_by_class) < 2:\n",
    "        print(\"Need at least 2 classes for interpolation\")\n",
    "        return\n",
    "    \n",
    "    # Encode samples to latent space\n",
    "    latent_codes = []\n",
    "    for i, sample in enumerate(samples_by_class):\n",
    "        condition = np.zeros((1, cvae.num_classes))\n",
    "        condition[0, i] = 1\n",
    "        condition = tf.constant(condition, dtype=tf.float32)\n",
    "        \n",
    "        z_mean, _ = cvae.encoder([sample, condition])\n",
    "        latent_codes.append(z_mean)\n",
    "    \n",
    "    # Interpolate between first two classes\n",
    "    z1, z2 = latent_codes[0], latent_codes[1]\n",
    "    n_steps = 5\n",
    "    alphas = np.linspace(0, 1, n_steps)\n",
    "    \n",
    "    # Create conditions for interpolation (gradually change from class 0 to class 1)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        # Interpolate latent code\n",
    "        z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "        \n",
    "        # Interpolate condition (gradually change from class 0 to class 1)\n",
    "        condition = np.array([[1-alpha, alpha] + [0] * (cvae.num_classes-2)])\n",
    "        condition = tf.constant(condition, dtype=tf.float32)\n",
    "        \n",
    "        # Generate sample\n",
    "        generated = cvae.decode(z_interp, condition)\n",
    "        \n",
    "        plt.subplot(1, n_steps, i + 1)\n",
    "        plt.bar(range(9), generated[0])\n",
    "        plt.title(f\"α = {alpha:.2f}\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(range(9))\n",
    "    \n",
    "    plt.suptitle(\"Interpolation Between Conditions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_cvae_summary(cvae):\n",
    "    \"\"\"Print CVAE architecture summary\"\"\"\n",
    "    print(\"CVAE Architecture:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Input dimension: {cvae.input_dim}\")\n",
    "    print(f\"Hidden dimension: {cvae.hidden_dim}\")\n",
    "    print(f\"Latent dimension: {cvae.latent_dim}\")\n",
    "    print(f\"Number of classes: {cvae.num_classes}\")\n",
    "    print(\"\\nEncoder (takes data + condition):\")\n",
    "    cvae.encoder.summary()\n",
    "    print(\"\\nDecoder (takes latent + condition):\")\n",
    "    cvae.decoder.summary()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate conditional synthetic data\n",
    "    print(\"Generating synthetic conditional 9D data...\")\n",
    "    num_classes = 3\n",
    "    data, labels = generate_conditional_synthetic_data(n_samples=1800, num_classes=num_classes)\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_size = int(0.8 * len(data))\n",
    "    x_train, x_test = data[:train_size], data[train_size:]\n",
    "    y_train, y_test = labels[:train_size], labels[train_size:]\n",
    "    \n",
    "    # Convert labels to one-hot\n",
    "    y_train_onehot = labels_to_onehot(y_train, num_classes)\n",
    "    y_test_onehot = labels_to_onehot(y_test, num_classes)\n",
    "    \n",
    "    # Create dataset\n",
    "    batch_size = 64\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_onehot))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "    \n",
    "    # Create CVAE\n",
    "    cvae = CVAE(input_dim=9, hidden_dim=7, latent_dim=2, num_classes=num_classes)\n",
    "    print_cvae_summary(cvae)\n",
    "    \n",
    "    # Train CVAE\n",
    "    trainer = CVAETrainer(cvae)\n",
    "    print(\"\\nTraining CVAE...\")\n",
    "    trainer.train(train_dataset, epochs=100)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\nPlotting conditional reconstructions...\")\n",
    "    plot_conditional_reconstructions(cvae, x_test, y_test)\n",
    "    \n",
    "    print(\"Generating conditional samples...\")\n",
    "    generate_conditional_samples(cvae)\n",
    "    \n",
    "    print(\"Plotting conditional latent space...\")\n",
    "    plot_conditional_latent_space(cvae, x_test, y_test)\n",
    "    \n",
    "    print(\"Interpolating between conditions...\")\n",
    "    interpolate_between_conditions(cvae, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
